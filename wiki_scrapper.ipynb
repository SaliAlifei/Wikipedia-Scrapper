{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wiki_scrapper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-0YKjPVaSkC"
      },
      "source": [
        "# **Wikipedia Scrapper**\n",
        "\n",
        "Permet d'ajouter une description aux éléments d'un fichier depuis la page wikipedia dediée. <br>\n",
        "\n",
        "Le fichier doit etre formaté comme suit :\n",
        "  - Pas d'en-tete ni d'index\n",
        "  - Première colonne : identifiant ou laisser vide\n",
        "  - Deuxième colonne : nom ou laisser vide\n",
        "  - Troisième colonne : nom de l'élément recherché separé par un underscrore (par exemple : michael_jackson)\n",
        "  - Quatrième colonne : Laisser vide. C'est ici que seront stockées les nouvelles descriptions <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzDPkYCXbsuJ"
      },
      "source": [
        "Dans le code ci-dessous, veuillez changer les variables ci-dessous afin qu'elles correspondent à vos fichiers.\n",
        "\n",
        "  - ***nom_fichier_csv*** : nom du fichier d'entrée\n",
        "  - ***dossier*** : chemin du dossier dans le lequel se trouve le fichier d'entrée\n",
        "  - ***nom_nouveau_fichier*** : nom du fichier de sortie \n",
        "  - ***defaut*** : il se peut qu'il n'existe pas de page Wikipedia pour certains éléments de votre fichier. La variable defaut sert donc de texte par defaut dans ces cas là. <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiW46YzVaIAz"
      },
      "source": [
        "nom_nouveau_fichier = \"chanteurs_description.csv\"\n",
        "dossier = \"/content/\"\n",
        "nom_fichier_csv =\"chanteurs.csv\"\n",
        "defaut = \"vide\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80f31QN3kVul"
      },
      "source": [
        "Par defaut, si une page Wikipedia n'existe pas en français pour élément donné, c'est la page en anglais (si elle existe) qui sera ajoutée comme description. <br>\n",
        "\n",
        "Si cela ne vous correspond pas, changez la variable ci-dessous par \"False\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qucYd04ekVUS"
      },
      "source": [
        "description_en_anglais = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-DfbCnUb3dV"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as plt\n",
        "import json\n",
        "import urllib.request\n",
        "\n",
        "# Liens nécessaires à l'utilisation de l'API de Wikipedia\n",
        "\n",
        "url_fr = \"https://fr.wikipedia.org/w/api.php?action=opensearch&format=json&search=\"\n",
        "url_an = \"https://en.wikipedia.org/w/api.php?action=opensearch&format=json&search=\"\n",
        "url_result_an = \"https://en.wikipedia.org/w/api.php?action=query&prop=extracts&exintro&explaintext&redirects=1&format=json&titles=\"\n",
        "url_result_fr = \"https://fr.wikipedia.org/w/api.php?action=query&prop=extracts&exintro&explaintext&redirects=1&format=json&titles=\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZIHfc0RcISC"
      },
      "source": [
        "# Lecture du fichier csv et transformation en tableau numpy\n",
        "file = pd.read_csv(dossier+nom_fichier_csv, header=None, names=[\"id\", \"nom\", \"nom_fichier\", \"description\"])\n",
        "array = np.asarray(file)\n",
        "\n",
        "\n",
        "# Permet de récuperer le fichier json correspondant à la page Wikipedia de nom_fichier\n",
        "def getJson(nom_fichier, url_langue):\n",
        "    URL = url_langue + nom_fichier\n",
        "    with urllib.request.urlopen(URL) as url:\n",
        "        data = json.loads(url.read().decode())\n",
        "    return data\n",
        "\n",
        "\n",
        "for line in array:\n",
        "    if (getJson(line[2].capitalize(), url_fr)[3] == []) and description_en_anglais : # si la page francaise pour cette ligne est vide et si la description_en_anglais est activée\n",
        "        line[3] = getJson(line[2].capitalize(), url_an)\n",
        "    else :\n",
        "        line[3] = getJson(line[2].capitalize(), url_fr)\n",
        "\n",
        "\n",
        "def getInfo(liste_json):\n",
        "    name = liste_json[0]\n",
        "    if \"https://en.\" in liste_json[3][0]:\n",
        "        data = getJson(name, url_result_an)\n",
        "    else:\n",
        "        data = getJson(name, url_result_fr)\n",
        "    return data\n",
        "\n",
        "\n",
        "# Affiche les fichiers json de manière formatée\n",
        "def printJson(dat):\n",
        "    print(json.dumps(dat, indent=4)) \n",
        "\n",
        "\n",
        "# Récupere le contenu depuis wikipedia\n",
        "def content (list_json):\n",
        "    key = list(list_json[\"query\"][\"pages\"].keys())[0]\n",
        "    try:\n",
        "        text = list_json[\"query\"][\"pages\"][key][\"extract\"]\n",
        "        tokens = text.replace(\"\\n\", \".\").replace(\"\\t\",\"\")\n",
        "        return tokens\n",
        "    except KeyError:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "# Remplace les colonnes description du fichier d'entrée par les contenus Wikipedia\n",
        "# Si aucune page n'a été trouvée pour un élément, la case sera remplie par defaut\n",
        "for line in array:\n",
        "    if line[3][3] == []:\n",
        "        line[3] = \"vide\"\n",
        "    else:\n",
        "        lis_json = getInfo(line[3])\n",
        "        line[3] = content(lis_json)  \n",
        "\n",
        "\n",
        "# Enregistre le nouveau fichier dans le même repertoire que le fichier de base\n",
        "pd.DataFrame(array).to_csv(dossier+nom_nouveau_fichier, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}